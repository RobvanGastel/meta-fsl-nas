{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import scipy.special\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/rob/Git/meta-fsl-nas/metanas\")\n",
    "\n",
    "import metanas.utils.genotypes as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Model for a single DARTS cell\"\"\"\n",
    "    def __init__(self, n_ops=7, n_nodes=3):\n",
    "        self.n_ops = len(gt.PRIMITIVES_FEWSHOT)\n",
    "        self.n_nodes = 3\n",
    "\n",
    "        \n",
    "        self.encoded_states = []\n",
    "        self.states = []\n",
    "        self.topk = []\n",
    "        \n",
    "        self.alphas = []\n",
    "        self.norm_alphas = []\n",
    "\n",
    "        # Adjacency matrix\n",
    "        self.A = np.ones((self.n_nodes+2, self.n_nodes+2)) - np.eye(self.n_nodes+2)\n",
    "\n",
    "        # Remove the 2 input nodes from A\n",
    "        self.A[0, 1] = 0\n",
    "        self.A[1, 0] = 0\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            a = nn.Parameter(\n",
    "                1e-3 * torch.randn(i + 2, n_ops))\n",
    "            self.alphas.append(a)\n",
    "            self.norm_alphas.append(F.softmax(a, dim=-1))\n",
    "\n",
    "    def print_topk(self):\n",
    "        for i, edges in enumerate(self.norm_alphas):\n",
    "            # edges: Tensor(n_edges, n_ops)\n",
    "            edge_max, _ = torch.topk(edges[:, :], 1)\n",
    "            # selecting the top-k input nodes, k=2\n",
    "            _, topk_edge_indices = torch.topk(edge_max.view(-1), k=2)\n",
    "            \n",
    "            print(topk_edge_indices)\n",
    "    \n",
    "    def parse(self, alpha, k=2, primitives=gt.PRIMITIVES_FEWSHOT):\n",
    "        gene = []\n",
    "        for edges in alpha:\n",
    "            edge_max, primitive_indices = torch.topk(\n",
    "                edges[:, :], 1\n",
    "            )\n",
    "            \n",
    "#             print(edges[:,:], primitive_indices, edge_max, \"\\n\")\n",
    "\n",
    "            topk_edge_values, topk_edge_indices = torch.topk(\n",
    "                edge_max.view(-1), k)\n",
    "\n",
    "#             print(topk_edge_values, topk_edge_indices, \"\\n\")\n",
    "            \n",
    "            node_gene = []\n",
    "            for edge_idx in topk_edge_indices:\n",
    "                prim_idx = primitive_indices[edge_idx]\n",
    "                prim = primitives[prim_idx]\n",
    "                node_gene.append((prim, edge_idx.item()))\n",
    "\n",
    "            gene.append(node_gene)\n",
    "        return gene\n",
    "    \n",
    "    def calculate_states(self):\n",
    "        s_idx = 0\n",
    "        \n",
    "#         print(self.states)\n",
    "#         if current_states is not None:\n",
    "        prev_topk = copy.deepcopy(self.topk)\n",
    "        prev_edge = copy.deepcopy(self.encoded_states)\n",
    "        \n",
    "        self.topk = []\n",
    "        self.states = []\n",
    "        self.encoded_states = []\n",
    "        self.edge_to_index = {}\n",
    "        self.edge_to_alpha = {}\n",
    "\n",
    "        for i, edges in enumerate(self.norm_alphas):\n",
    "            # edges: Tensor(n_edges, n_ops)\n",
    "            edge_max, edge_idx = torch.topk(edges[:, :], 1)\n",
    "            \n",
    "            # selecting the top-k input nodes, k=2\n",
    "            _, topk_edge_indices = torch.topk(edge_max.view(-1), k=2)\n",
    "\n",
    "            edge_one_hot = torch.zeros_like(edges[:,:])\n",
    "            \n",
    "            \n",
    "            for hot_e, op in zip(edge_one_hot, edge_idx):\n",
    "                hot_e[op.item()] = 1\n",
    "\n",
    "            for j, edge in enumerate(edge_one_hot):\n",
    "                self.edge_to_index[(j, i+2)] = s_idx\n",
    "                self.edge_to_index[(i+2, j)] = s_idx+1\n",
    "\n",
    "                self.edge_to_alpha[(j, i+2)] = (i, j)\n",
    "                self.edge_to_alpha[(i+2, j)] = (i, j)\n",
    "\n",
    "                self.encoded_states.append(edge.numpy())\n",
    "                \n",
    "                # For undirected edge we add the edge twice\n",
    "                self.states.append([\n",
    "                        f\"from:{j} to:{i+2}\",\n",
    "                        int(j in topk_edge_indices)])\n",
    "                \n",
    "                self.topk.append([\n",
    "                        f\"from:{j} to:{i+2}\",\n",
    "                        int(j in topk_edge_indices)])\n",
    "\n",
    "#                 self.states.append((\n",
    "#                         (f\"from:{i+2}\",\n",
    "#                         f\"to:{j}\"),\n",
    "#                         [int(j in topk_edge_indices)]))\n",
    "                s_idx += 2\n",
    "    \n",
    "        d = {'prev_topk': np.array(prev_topk),\n",
    "             'prev_edges': np.array(prev_edge)}\n",
    "    \n",
    "        self.encoded_states = np.array(self.encoded_states)\n",
    "#         change = (np.array(prev_topk) < np.array(self.topk))\n",
    "        return d, self.states\n",
    "    \n",
    "    def _inverse_softmax(self, x, C):\n",
    "        return torch.log(x) + C\n",
    "    \n",
    "    def increase_op(self, cur_node, next_node, op_idx, prob=0.1, n_ops=7):\n",
    "#         t_max = 5.0\n",
    "#         t_min = 0.1\n",
    "#         max_step = 6\n",
    "#         curr_step = 1\n",
    "#         # Temperature\n",
    "#         temp = t_max - curr_step * (t_max - t_min)/max_step-1\n",
    "\n",
    "        C = math.log(10.)\n",
    "\n",
    "        row_idx, edge_idx = self.edge_to_alpha[(cur_node, next_node)]\n",
    "        \n",
    "        # Set short-hands\n",
    "        curr_op = self.norm_alphas[row_idx][edge_idx][op_idx]\n",
    "        curr_edge = self.norm_alphas[row_idx][edge_idx]\n",
    "        \n",
    "        \n",
    "        # Allow for increasing to 0.99\n",
    "        if curr_op + prob > 1.0:\n",
    "            surplus = curr_op + prob - 0.99\n",
    "            prob -= surplus\n",
    "\n",
    "        if curr_op + prob < 1.0:\n",
    "            # Increase chosen op\n",
    "            with torch.no_grad():\n",
    "                curr_op += prob\n",
    "\n",
    "            # Prevent 0.00 normalized alpha values, resulting in\n",
    "            # -inf\n",
    "            with torch.no_grad():\n",
    "                curr_edge += 0.01\n",
    "\n",
    "            # Set the meta-model, update the env state in\n",
    "            # self.update_states()\n",
    "            with torch.no_grad():\n",
    "                self.alphas[\n",
    "                    row_idx][edge_idx] = self._inverse_softmax(\n",
    "                    curr_edge, C)\n",
    "        \n",
    "        # /temp\n",
    "        self.norm_alphas = [\n",
    "            F.softmax(alpha, dim=-1).detach().cpu()\n",
    "            for alpha in self.alphas]\n",
    "    \n",
    "    def decrease_op(self, cur_node, next_node, op_idx, prob=0.1, n_ops=7):\n",
    "        C = math.log(10.)\n",
    "\n",
    "        row_idx, edge_idx = self.edge_to_alpha[(cur_node, next_node)]\n",
    "        \n",
    "        # Set short-hands\n",
    "        curr_op = self.norm_alphas[row_idx][edge_idx][op_idx]\n",
    "        curr_edge = self.norm_alphas[row_idx][edge_idx]\n",
    "        \n",
    "        \n",
    "        # Allow for increasing to 0.99\n",
    "        if curr_op - prob < 0.0:\n",
    "            surplus = prob - curr_op + 0.01\n",
    "#             print(surplus)\n",
    "            prob -= surplus\n",
    "#             print(prob)\n",
    "\n",
    "        if curr_op - prob > 0.0:\n",
    "            # Increase chosen op\n",
    "            with torch.no_grad():\n",
    "                curr_op -= prob\n",
    "                \n",
    "            # Prevent 0.00 normalized alpha values, resulting in\n",
    "            # -inf\n",
    "            with torch.no_grad():\n",
    "                curr_edge += 0.01\n",
    "            \n",
    "            # Set the meta-model, update the env state in\n",
    "            # self.update_states()\n",
    "            with torch.no_grad():\n",
    "                self.alphas[\n",
    "                    row_idx][edge_idx] = self._inverse_softmax(\n",
    "                    curr_edge, C)\n",
    "            \n",
    "        self.norm_alphas = [\n",
    "            F.softmax(alpha, dim=-1).detach().cpu()\n",
    "            for alpha in self.alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primitives: ['max_pool_3x3', 'avg_pool_3x3', 'skip_connect', 'conv_1x5_5x1', 'conv_3x3', 'sep_conv_3x3', 'dil_conv_3x3'] \n",
      "\n",
      "tensor([1, 0])\n",
      "tensor([0, 2])\n",
      "tensor([2, 0])\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "print(\"primitives:\", gt.PRIMITIVES_FEWSHOT, \"\\n\")\n",
    "model.print_topk(), model.parse(model.norm_alphas, k=2)\n",
    "\n",
    "_, b = model.calculate_states()\n",
    "# print(\"init:\", b)\n",
    "\n",
    "# print(model.norm_alphas[1])\n",
    "\n",
    "# d, _ = model.calculate_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1428, 0.1427, 0.1429, 0.1430, 0.1430, 0.1426, 0.1430],\n",
       "         [0.1427, 0.1428, 0.1428, 0.1430, 0.1430, 0.1430, 0.1427]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1428, 0.1431, 0.1426, 0.1429, 0.1430, 0.1429, 0.1428],\n",
       "         [0.1429, 0.1429, 0.1430, 0.1427, 0.1429, 0.1429, 0.1427],\n",
       "         [0.1427, 0.1429, 0.1430, 0.1428, 0.1429, 0.1429, 0.1428]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1429, 0.1428, 0.1428, 0.1431, 0.1427, 0.1430, 0.1427],\n",
       "         [0.1429, 0.1429, 0.1429, 0.1428, 0.1429, 0.1427, 0.1429],\n",
       "         [0.1429, 0.1429, 0.1432, 0.1426, 0.1428, 0.1428, 0.1428],\n",
       "         [0.1430, 0.1429, 0.1428, 0.1428, 0.1431, 0.1425, 0.1429]],\n",
       "        grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.norm_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from:0 to:2', 1],\n",
       " ['from:1 to:2', 1],\n",
       " ['from:0 to:3', 1],\n",
       " ['from:1 to:3', 0],\n",
       " ['from:2 to:3', 1],\n",
       " ['from:0 to:4', 1],\n",
       " ['from:1 to:4', 0],\n",
       " ['from:2 to:4', 1],\n",
       " ['from:3 to:4', 0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from:0 to:2', 1],\n",
       " ['from:1 to:2', 1],\n",
       " ['from:0 to:3', 1],\n",
       " ['from:1 to:3', 0],\n",
       " ['from:2 to:3', 1],\n",
       " ['from:0 to:4', 1],\n",
       " ['from:1 to:4', 0],\n",
       " ['from:2 to:4', 1],\n",
       " ['from:3 to:4', 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.increase_op(0, 3, 5)\n",
    "model.increase_op(0, 3, 5)\n",
    "\n",
    "model.calculate_states()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1428, 0.1427, 0.1429, 0.1430, 0.1430, 0.1426, 0.1430],\n",
       "         [0.1427, 0.1428, 0.1428, 0.1430, 0.1430, 0.1430, 0.1427]]),\n",
       " tensor([[0.1306, 0.1308, 0.1304, 0.1307, 0.1307, 0.2161, 0.1306],\n",
       "         [0.1429, 0.1429, 0.1430, 0.1427, 0.1429, 0.1429, 0.1427],\n",
       "         [0.1427, 0.1429, 0.1430, 0.1428, 0.1429, 0.1429, 0.1428]]),\n",
       " tensor([[0.1429, 0.1428, 0.1428, 0.1431, 0.1427, 0.1430, 0.1427],\n",
       "         [0.1429, 0.1429, 0.1429, 0.1428, 0.1429, 0.1427, 0.1429],\n",
       "         [0.1429, 0.1429, 0.1432, 0.1426, 0.1428, 0.1428, 0.1428],\n",
       "         [0.1430, 0.1429, 0.1428, 0.1428, 0.1431, 0.1425, 0.1429]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.norm_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 12, 32, 12], [321, 312, 12]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in self.workers:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0])\n",
      "tensor([1, 2])\n",
      "tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "model.print_topk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1430, 0.1432, 0.1426, 0.1429, 0.1426, 0.1427, 0.1430],\n",
      "        [0.1429, 0.1428, 0.1427, 0.1427, 0.1431, 0.1427, 0.1431],\n",
      "        [0.1426, 0.1429, 0.1428, 0.1428, 0.1430, 0.1429, 0.1430]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.norm_alphas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    a = np.random.randint(4, 5)\n",
    "    b = np.random.randint(0, len(gt.PRIMITIVES_FEWSHOT))\n",
    "    model.increase_op(a, 3, b)\n",
    "    model.increase_op(a, 3, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1426, 0.1429, 0.1429, 0.1430, 0.1430, 0.1428, 0.1429],\n",
      "        [0.0248, 0.0150, 0.0420, 0.5894, 0.0915, 0.0163, 0.2210],\n",
      "        [0.0150, 0.0914, 0.0150, 0.5899, 0.0248, 0.0429, 0.2210]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.norm_alphas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.increase_op(1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1426, 0.1429, 0.1429, 0.1430, 0.1430, 0.1428, 0.1429],\n",
      "        [0.0303, 0.7786, 0.0312, 0.0569, 0.0335, 0.0299, 0.0396],\n",
      "        [0.0150, 0.0914, 0.0150, 0.5899, 0.0248, 0.0429, 0.2210]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model.norm_alphas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import shelve\n",
    "\n",
    "import igraph as ig\n",
    "from igraph import Graph\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph_path(path, last_steps=None, paths_left=5):\n",
    "    d = shelve.open(path)\n",
    "    walks = sum(d.values(), [])\n",
    "    d.close()\n",
    "    \n",
    "    if last_steps is not None:\n",
    "        walks = walks[:last_steps]\n",
    "        \n",
    "    # TODO: Starting path might be variable\n",
    "    path = [(0,2)]\n",
    "    weights = [1]\n",
    "\n",
    "    walks_temp = []\n",
    "    walks_curr = copy.deepcopy(walks)\n",
    "    max_k = len(walks)\n",
    "\n",
    "    for i in range(max_k):\n",
    "        edge_dict = {}\n",
    "        walks_temp = []\n",
    "\n",
    "        for j, walk in enumerate(walks_curr):\n",
    "            # Check if current walk is long enough\n",
    "            if i >= len(walk):\n",
    "                continue\n",
    "            else:\n",
    "                # Current step\n",
    "                w = walk[i]\n",
    "\n",
    "                if w in edge_dict:\n",
    "                    edge_dict[w] += 1\n",
    "                else:\n",
    "                    edge_dict[w] = 1\n",
    "\n",
    "                walks_temp.append(walk)\n",
    "\n",
    "        # Stop if the path ended or,\n",
    "        if len(edge_dict) == 0:\n",
    "            break\n",
    "        # Or if only 5 walks are left \n",
    "        if sum([v for v in edge_dict.values()]) < paths_left:\n",
    "            break\n",
    "\n",
    "        # Step with highest count\n",
    "        max_edge = max(edge_dict, key=edge_dict.get)\n",
    "        path.append(max_edge)\n",
    "        weights.append(edge_dict[max_edge]/(sum(edge_dict.values())))\n",
    "\n",
    "        for walk in walks_temp:\n",
    "            if walk[i] != max_edge:\n",
    "                walks_temp.remove(walk)\n",
    "        walks_curr = copy.deepcopy(walks_temp)\n",
    "        \n",
    "        return path, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(path, weights, save_paths, format_path):\n",
    "    for f in save_paths:\n",
    "        os.remove(f)\n",
    "\n",
    "    edges = [(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "    edge_color = [\"gray\"]*len(edges)\n",
    "\n",
    "    for i, (edge, weight) in enumerate(zip(path, weights)):\n",
    "        edges = [(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "        g = Graph(edges)\n",
    "\n",
    "        if edge in edges:\n",
    "            index = edges.index(edge)\n",
    "        else:\n",
    "            index = edges.index((edge[1], edge[0]))\n",
    "\n",
    "        edge_color[index] = 'red'\n",
    "\n",
    "        lb = [\"\"]*len(edges)\n",
    "        lb[index] = f\"step {i}: {edge[0]} -> {edge[1]}, weight: {weight:.2f}\"\n",
    "\n",
    "        # 5 Nodes\n",
    "        g.vs[\"label\"] = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "        g.vs[\"input\"] = [True, True, False, False, False]\n",
    "        g.es[\"color\"] = edge_color\n",
    "        g.es[\"label\"] = lb\n",
    "\n",
    "        ig.plot(\n",
    "            g, \n",
    "            vertex_size=40, \n",
    "            edge_width=[3],\n",
    "            vertex_color=['yellow', 'yellow', 'blue', 'blue', 'blue'],\n",
    "            target=format_path.format(i),\n",
    "            bbox=(800, 800),\n",
    "            margin=200\n",
    "        )\n",
    "        edge_color[index] = 'purple'\n",
    "\n",
    "    frames = []\n",
    "    for img in sorted(glob.glob(save_paths), key=os.path.getmtime):\n",
    "        frames.append(Image.open(img))\n",
    "\n",
    "    frames[0].save('graph_walk.gif', format='GIF', append_images=frames[1:],\n",
    "        save_all=True, duration=1800, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/rob/Git/meta-fsl-nas/metanas/results/triplemnist/ppo_metad2a_environment_1/seed_2/graph_walk.shlv\"\n",
    "save_paths = glob.glob(\"/home/rob/Git/meta-fsl-nas/notebooks/path/*.png\")\n",
    "format_path = \"/home/rob/Git/meta-fsl-nas/notebooks/path/{0}.png\"\n",
    "\n",
    "path, weights = generate_graph_path(path)\n",
    "generate_gif(path, weights, save_paths, format_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_avg_mean_std_dataset(path):\n",
    "    mean_sum = np.array([0., 0., 0.])\n",
    "    std_sum = np.array([0., 0., 0.])\n",
    "    \n",
    "    n_images = 0\n",
    "    for file in list(glob.glob(path)):\n",
    "        img = cv.imread(file)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = img/255\n",
    "        mean, std = cv.meanStdDev(img)\n",
    "        \n",
    "        mean_sum += np.squeeze(mean)\n",
    "        std_sum += np.squeeze(std)\n",
    "        n_images += 1\n",
    "    return (mean_sum / n_images, std_sum / n_images)\n",
    "\n",
    "path = '/home/rob/Desktop/meta5/*/*/*.png'\n",
    "\n",
    "mean, std = calc_avg_mean_std_dataset(path)\n",
    "print(mean, std)\n",
    "\n",
    "print(\"{:0.4f}, {:0.4f}\".format(mean[0], std[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Load model back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_path = '/home/rob/Git/meta-fsl-nas/metanas/results/omniprint/ppo_debug/seed_2/_s2/vars1.pkl'\n",
    "model_path = '/home/rob/Git/meta-fsl-nas/metanas/results/omniprint/ppo_debug/seed_2/_s2/pyt_save/model1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(torch.load(model_path)['ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open(vars_path, 'rb'))\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchmeta.utils.data import Dataset, ClassDataset, CombinationMetaDataset\n",
    "from torchmeta.datasets.utils import download_file_from_google_drive\n",
    "from torchmeta.datasets.helpers import helper_with_default\n",
    "\n",
    "\n",
    "def omniprint(folder, shots, ways, shuffle=True, test_shots=None,\n",
    "              seed=None, **kwargs):\n",
    "    return helper_with_default(OmniPrint, folder, shots, ways,\n",
    "                               shuffle=shuffle, test_shots=test_shots,\n",
    "                               seed=seed, **kwargs)\n",
    "\n",
    "\n",
    "class OmniPrint(CombinationMetaDataset):\n",
    "    def __init__(self, root, num_classes_per_task=None, meta_train=False,\n",
    "                 meta_val=False, meta_test=False, meta_split=None,\n",
    "                 transform=None, target_transform=None, dataset_transform=None,\n",
    "                 class_augmentations=None, download=False,\n",
    "                 print_split='meta1',  # Addition for the OmniPrint dataset\n",
    "                 ):\n",
    "        dataset = OmniPrintClassDataset(\n",
    "            root, meta_train=meta_train,\n",
    "            meta_val=meta_val, meta_test=meta_test,\n",
    "            print_split=print_split, transform=transform,\n",
    "            meta_split=meta_split,\n",
    "            class_augmentations=class_augmentations,\n",
    "            download=download)\n",
    "        super(OmniPrint, self).__init__(\n",
    "            dataset, num_classes_per_task,\n",
    "            target_transform=target_transform,\n",
    "            dataset_transform=dataset_transform)\n",
    "\n",
    "\n",
    "class OmniPrintClassDataset(ClassDataset):\n",
    "    gdrive_id = '1JBXYMTsdlm8RaEBPqrJbDRzs3hJ4q_gH'\n",
    "    folder = 'omniprint'\n",
    "\n",
    "    zip_filename = '{0}.zip'\n",
    "    filename = '{0}_{1}_data.hdf5'\n",
    "    filename_labels = '{0}_{1}_labels.json'\n",
    "\n",
    "    def __init__(self, root, meta_train=False, meta_val=False, meta_test=False,\n",
    "                 meta_split=None, print_split=None, transform=None,\n",
    "                 class_augmentations=None, download=False):\n",
    "        super(OmniPrintClassDataset, self).__init__(\n",
    "            meta_train=meta_train,\n",
    "            meta_val=meta_val,\n",
    "            meta_test=meta_test,\n",
    "            meta_split=meta_split,\n",
    "            class_augmentations=class_augmentations)\n",
    "\n",
    "        self.root = os.path.join(os.path.expanduser(\n",
    "            root), self.folder)\n",
    "        self.print_split = print_split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.split_filename = os.path.join(\n",
    "            self.root,\n",
    "            self.filename.format(print_split, self.meta_split))\n",
    "        self.split_filename_labels = os.path.join(\n",
    "            self.root,\n",
    "            self.filename_labels.format(print_split, self.meta_split))\n",
    "\n",
    "        self._data = None\n",
    "        self._labels = None\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('OmniPrint integrity check failed')\n",
    "        self._num_classes = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        character_name = '/'.join(self.labels[index % self.num_classes])\n",
    "        data = self.data[character_name]\n",
    "        transform = self.get_transform(index, self.transform)\n",
    "        target_transform = self.get_target_transform(index)\n",
    "\n",
    "        return OmniPrintDataset(\n",
    "            index, data, character_name,\n",
    "            transform=transform, target_transform=target_transform)\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._data is None:\n",
    "            self._data = h5py.File(self.split_filename, 'r')\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if self._labels is None:\n",
    "            with open(self.split_filename_labels, 'r') as f:\n",
    "                self._labels = json.load(f)\n",
    "        return self._labels\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        return (os.path.isfile(self.split_filename)\n",
    "                and os.path.isfile(self.split_filename_labels))\n",
    "\n",
    "    def close(self):\n",
    "        if self._data is not None:\n",
    "            self._data.close()\n",
    "            self._data = None\n",
    "\n",
    "    def download(self):\n",
    "        import zipfile\n",
    "        import shutil\n",
    "        import glob\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        if self._check_integrity():\n",
    "            return\n",
    "\n",
    "        zip_foldername = os.path.join(\n",
    "            self.root, self.zip_filename.format(self.folder))\n",
    "        # Download the datasets\n",
    "        if not os.path.isfile(zip_foldername):\n",
    "            download_file_from_google_drive(\n",
    "                self.gdrive_id, self.root,\n",
    "                self.zip_filename.format(self.folder))\n",
    "\n",
    "        # Unzip the dataset\n",
    "        if not os.path.isdir(zip_foldername):\n",
    "            with zipfile.ZipFile(zip_foldername) as f:\n",
    "                for member in tqdm(f.infolist(), desc='Extracting '):\n",
    "                    try:\n",
    "                        f.extract(member, self.root)\n",
    "                    except zipfile.BadZipFile:\n",
    "                        print('Error: Zipfile is corrupted')\n",
    "\n",
    "        for print_split in ['meta1', 'meta2', 'meta3', 'meta4', 'meta5']:\n",
    "            for split in tqdm(['train', 'val', 'test'], desc=print_split):\n",
    "                filename_labels = os.path.join(\n",
    "                    self.root, self.filename_labels.format(print_split, split))\n",
    "\n",
    "                with open(filename_labels, 'r') as f:\n",
    "                    labels = json.load(f)\n",
    "\n",
    "                filename = os.path.join(\n",
    "                    self.root, self.filename.format(print_split, split))\n",
    "\n",
    "                with h5py.File(filename, 'w') as f:\n",
    "                    group = f.create_group(print_split)\n",
    "                    for _, alphabet, character in labels:\n",
    "                        filenames = glob.glob(\n",
    "                            os.path.join(\n",
    "                                self.root, print_split,\n",
    "                                alphabet, character, '*.png'))\n",
    "                        dataset = group.create_dataset('{0}/{1}'.format(\n",
    "                            alphabet, character),\n",
    "                            (len(filenames), 32, 32),\n",
    "                            dtype='uint8')\n",
    "\n",
    "                        for i, char_filename in enumerate(filenames):\n",
    "                            image = Image.open(\n",
    "                                char_filename, mode='r').convert('L')\n",
    "                            dataset[i] = image\n",
    "\n",
    "            shutil.rmtree(os.path.join(self.root, print_split))\n",
    "\n",
    "\n",
    "class OmniPrintDataset(Dataset):\n",
    "    def __init__(self, index, data, character_name,\n",
    "                 transform=None, target_transform=None):\n",
    "        super(OmniPrintDataset, self).__init__(\n",
    "            index, transform=transform,\n",
    "            target_transform=target_transform)\n",
    "        self.data = data\n",
    "        self.character_name = character_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.fromarray(self.data[index])\n",
    "        target = self.character_name\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return (image, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "\n",
    "dataset = omniprint(\n",
    "        \"/home/rob/Desktop\",\n",
    "        15,\n",
    "        5,\n",
    "        print_split='meta1',\n",
    "        meta_split='val',\n",
    "        test_shots=1,\n",
    "        download=True,\n",
    "        seed=1,\n",
    ")\n",
    "\n",
    "dataloader = BatchMetaDataLoader(\n",
    "    dataset, batch_size=20, num_workers=1, shuffle=True\n",
    ")\n",
    "\n",
    "train_it = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(next(iter(dataloader))['train'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 75, 1, 32, 32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_it)['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 32, 32])\n",
      "torch.Size([15, 1, 32, 32])\n",
      "torch.Size([5, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Task(train_loader=<torch.utils.data.dataloader.DataLoader object at 0x7fda153230b8>, valid_loader=<torch.utils.data.dataloader.DataLoader object at 0x7fda15323208>, test_loader=<torch.utils.data.dataloader.DataLoader object at 0x7fda147bc550>)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "Task = namedtuple(\"Task\", [\"train_loader\", \"valid_loader\", \"test_loader\"])\n",
    "\n",
    "meta_batch_size = 1\n",
    "task_batch_size = 15*20\n",
    "shots = 5\n",
    "ways = 5\n",
    "validation_set = True\n",
    "\n",
    "batch = next(train_it)\n",
    "train_batch_x, train_batch_y = batch[\"train\"]\n",
    "test_batch_x, test_batch_y = batch[\"test\"]\n",
    "num_tasks = meta_batch_size\n",
    "\n",
    "meta_train_batch = list()\n",
    "for task_idx in range(num_tasks):\n",
    "    \n",
    "    # Parse validation set\n",
    "    train_idx, valid_idx = train_test_split(\n",
    "        np.arange(len(train_batch_y[task_idx].numpy())),\n",
    "        test_size=0.2, random_state=42, shuffle=True,\n",
    "        stratify=train_batch_y[task_idx].numpy())\n",
    "    ####\n",
    "    \n",
    "    # Train loader\n",
    "    dset_train = TensorDataset(train_batch_x[task_idx][train_idx], train_batch_y[task_idx][train_idx])\n",
    "    train_loader = DataLoader(dset_train, batch_size=20)\n",
    "\n",
    "    # Validation loader\n",
    "    dset_val = TensorDataset(train_batch_x[task_idx][valid_idx], train_batch_y[task_idx][valid_idx])\n",
    "    val_loader = DataLoader(dset_val, batch_size=5)\n",
    "    \n",
    "    # Test loader\n",
    "    dset_test = TensorDataset(test_batch_x[task_idx], test_batch_y[task_idx])\n",
    "    test_loader = DataLoader(dset_test, batch_size=shots * ways)\n",
    "    \n",
    "    \n",
    "    print(train_batch_x[task_idx][train_idx].shape)\n",
    "    print(train_batch_x[task_idx][valid_idx].shape)\n",
    "    print(test_batch_x[task_idx].shape)\n",
    "    \n",
    "    meta_train_batch.append(Task(train_loader, val_loader, test_loader))\n",
    "    \n",
    "meta_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fda15323208>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train_batch[0].valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 32, 32])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(meta_train_batch[0].valid_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 32, 32]) torch.Size([20, 1, 32, 32])\n",
      "torch.Size([5, 1, 32, 32]) torch.Size([20, 1, 32, 32])\n",
      "torch.Size([5, 1, 32, 32]) torch.Size([20, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "task = meta_train_batch[0]\n",
    "\n",
    "for step, ((train_X, train_y), (val_X, val_y)) in enumerate(\n",
    "    zip(task.train_loader, task.valid_loader)):\n",
    "    print(val_X.shape, train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
